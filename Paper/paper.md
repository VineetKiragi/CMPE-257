---
title: Semester Project [rename]
date: "November 2022"
author: Carlos Rojas, San Jos√© State University

header-includes: |
  \usepackage{booktabs}
  \usepackage{caption}
---

# Abstract

# Tasks
- Devansh: work on BERT model and help on linear model.
- Rahul: work on BERT model and help more with report.
- Vineet: work on Linear model.

Pizza [@pizza2000identification] is an understudied yet widely utilized implement for delivering in-vivo *Solanum lycopersicum* based liquid mediums in a variety of next-generation mastications studies. Here we describe a de novo approach for large scale *T. aestivum* assemblies based on protein folding that drastically reduces the generation time of the mutation rate.

# Introduction (Vineet)

# Methods (All)

## Vineet

## Applying BERT Large Uncased Whole Word Masking with Squad Benchmarking (Devansh)

This technique uses an English language pre-trained model by employing masked language modeling (MLM) scheme. 
This model is not case-sensitive; and, it does not distinguish between `english` and `English`.

This BERT model was trained using a novel method called Whole Word Masking, unlike conventional BERT models. In this instance, a word's tokens are all simultaneously masked. Overall masking rate is unchanged.

Each masked WordPiece token is predicted separately; the training is the same.

After collecting the training data of cities in the US, we need to fine-tune our model to work well for this dataset. The format is maintained to be SQuAD-like.

This model has the following configuration:
- 24-layer
- 1024 hidden dimension
- 16 attention heads
- 336M parameters.

### Impact of Transfer Learning

In order to leverage the benefits of transfer learning, we must train the BERT pre-trained model again with our training dataset. This will update the weights to make the model's predictions relevant.


## Rahul

# Comparisons (Mainly linear vs BERT - Vineet/Devansh / Rahul for BERT comparison)

# Example Analysis (Devansh)

# Conclusions (Devansh)


# References
